#!/usr/bin/env python3
"""
ProTech Article Converter
ØªØ­ÙˆÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¬Ø¯ÙŠØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

import os
import re
from bs4 import BeautifulSoup
from datetime import datetime

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
OLD_ARTICLES_DIR = "old_articles"  # Ø¶Ø¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù‡Ù†Ø§
NEW_ARTICLES_DIR = "articles"       # Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø³ØªØ¸Ù‡Ø± Ù‡Ù†Ø§
TEMPLATE_PATH = "templates/article-template.html"
DATA_JS_PATH = "assets/js/data.js"

def extract_article_data(html_content, filename):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…"""
    soup = BeautifulSoup(html_content, 'html.parser')
    
    data = {
        'slug': filename.replace('.html', ''),
        'title': '',
        'description': '',
        'keywords': '',
        'category': 'tech',
        'date': datetime.now().strftime('%Y-%m-%d'),
        'image': '',
        'content': ''
    }
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù†
    title_tag = soup.find('title')
    if title_tag:
        data['title'] = title_tag.text.replace(' | ProTech', '').strip()
    else:
        h1_tag = soup.find('h1')
        if h1_tag:
            data['title'] = h1_tag.text.strip()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØµÙ
    desc_tag = soup.find('meta', {'name': 'description'})
    if desc_tag:
        data['description'] = desc_tag.get('content', '')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
    keywords_tag = soup.find('meta', {'name': 'keywords'})
    if keywords_tag:
        data['keywords'] = keywords_tag.get('content', '')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    og_image = soup.find('meta', {'property': 'og:image'})
    if og_image:
        data['image'] = og_image.get('content', '')
    else:
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„ ØµÙˆØ±Ø© ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        first_img = soup.find('img')
        if first_img:
            data['image'] = first_img.get('src', '')
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ®
    time_tag = soup.find('time')
    if time_tag:
        datetime_attr = time_tag.get('datetime')
        if datetime_attr:
            data['date'] = datetime_attr.split('T')[0]
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙØ¦Ø© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ© Ø£Ùˆ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    content_lower = html_content.lower()
    if 'iphone' in content_lower or 'ios' in content_lower or 'ipad' in content_lower:
        data['category'] = 'ios'
    elif 'ai' in content_lower or 'artificial intelligence' in content_lower or 'gemini' in content_lower:
        data['category'] = 'ai'
    elif 'ps5' in content_lower or 'ps6' in content_lower or 'xbox' in content_lower or 'gaming' in content_lower or 'nintendo' in content_lower:
        data['category'] = 'gaming'
    elif 'leak' in content_lower or 'rumor' in content_lower or 'ØªØ³Ø±ÙŠØ¨' in content_lower:
        data['category'] = 'leaks'
    elif 'samsung' in content_lower or 'galaxy' in content_lower or 'pixel' in content_lower:
        data['category'] = 'hardware'
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† div Ø£Ùˆ section Ø§Ù„ØªÙŠ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    article_content = None
    
    # Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø© Ù„Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    possible_containers = [
        soup.find('article'),
        soup.find('div', class_=re.compile('article|content|post|entry', re.I)),
        soup.find('section', class_=re.compile('article|content|post', re.I)),
        soup.find('main'),
    ]
    
    for container in possible_containers:
        if container:
            article_content = container
            break
    
    # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ container Ù…Ø­Ø¯Ø¯ØŒ Ù†Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨ÙŠÙ† header Ùˆ footer
    if not article_content:
        # Ø¥Ø²Ø§Ù„Ø© nav, header, footer
        for tag in soup.find_all(['nav', 'header', 'footer', 'script', 'style']):
            tag.decompose()
        
        # Ø£Ø®Ø° body ÙƒÙ…Ø­ØªÙˆÙ‰
        body = soup.find('body')
        if body:
            article_content = body
    
    if article_content:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        for tag in article_content.find_all(['nav', 'header', 'footer', 'script', 'style', 'iframe']):
            tag.decompose()
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø²Ø±Ø§Ø± ÙˆØ§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø§Ù†Ø¨ÙŠØ©
        for tag in article_content.find_all(class_=re.compile('share|social|sidebar|widget|ad|advertisement', re.I)):
            tag.decompose()
        
        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†Ø¸ÙŠÙ
        content_html = str(article_content)
        
        # ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠ
        content_html = re.sub(r'<div[^>]*class="[^"]*(?:share|social|sidebar)[^"]*"[^>]*>.*?</div>', '', content_html, flags=re.DOTALL)
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø¶Ù…Ù†Ø©
        content_html = re.sub(r' style="[^"]*"', '', content_html)
        
        # Ø¥Ø²Ø§Ù„Ø© classes ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©
        content_html = re.sub(r' class="[^"]*"', '', content_html)
        
        data['content'] = content_html
    
    return data

def format_date(date_str):
    """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ØªØ§Ø±ÙŠØ®"""
    try:
        date_obj = datetime.strptime(date_str, "%Y-%m-%d")
        return date_obj.strftime("%B %d, %Y")
    except:
        return date_str

def get_category_class(category):
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ class Ø§Ù„ÙØ¦Ø©"""
    category_map = {
        'ios': 'cat-ios',
        'ai': 'cat-ai',
        'leaks': 'cat-leaks',
        'hardware': 'cat-hardware',
        'gaming': 'cat-gaming',
        'tech': 'cat-tech'
    }
    return category_map.get(category.lower(), 'cat-tech')

def convert_article(old_html_path, template_path):
    """ØªØ­ÙˆÙŠÙ„ Ù…Ù‚Ø§Ù„ ÙˆØ§Ø­Ø¯"""
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù‚Ø¯ÙŠÙ…
    with open(old_html_path, 'r', encoding='utf-8') as f:
        old_html = f.read()
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù‚Ø§Ù„Ø¨
    with open(template_path, 'r', encoding='utf-8') as f:
        template = f.read()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    filename = os.path.basename(old_html_path)
    data = extract_article_data(old_html, filename)
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ÙÙŠ Ø§Ù„Ù‚Ø§Ù„Ø¨
    replacements = {
        '{{TITLE}}': data['title'],
        '{{DESCRIPTION}}': data['description'],
        '{{KEYWORDS}}': data['keywords'],
        '{{CANONICAL_URL}}': f"https://protech-news.vercel.app/articles/{data['slug']}.html",
        '{{IMAGE}}': data['image'],
        '{{DATE}}': data['date'],
        '{{DATE_FORMATTED}}': format_date(data['date']),
        '{{CATEGORY}}': data['category'].upper(),
        '{{CATEGORY_CLASS}}': get_category_class(data['category']),
        '{{TITLE_SHORT}}': data['title'][:50] + '...' if len(data['title']) > 50 else data['title'],
        '{{CONTENT}}': data['content']
    }
    
    new_html = template
    for placeholder, value in replacements.items():
        new_html = new_html.replace(placeholder, value)
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
    new_path = os.path.join(NEW_ARTICLES_DIR, filename)
    with open(new_path, 'w', encoding='utf-8') as f:
        f.write(new_html)
    
    return data

def convert_all_articles():
    """ØªØ­ÙˆÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª"""
    
    print("ğŸš€ ProTech Article Converter")
    print("=" * 60)
    
    # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
    if not os.path.exists(OLD_ARTICLES_DIR):
        print(f"\nâŒ Ø®Ø·Ø£: Ø§Ù„Ù…Ø¬Ù„Ø¯ {OLD_ARTICLES_DIR} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        print(f"   Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙˆÙˆØ¶Ø¹ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠÙ‡")
        return
    
    if not os.path.exists(TEMPLATE_PATH):
        print(f"\nâŒ Ø®Ø·Ø£: Ø§Ù„Ù‚Ø§Ù„Ø¨ {TEMPLATE_PATH} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    if not os.path.exists(NEW_ARTICLES_DIR):
        os.makedirs(NEW_ARTICLES_DIR)
    
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª HTML
    html_files = [f for f in os.listdir(OLD_ARTICLES_DIR) if f.endswith('.html')]
    
    if not html_files:
        print(f"\nâŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª HTML ÙÙŠ {OLD_ARTICLES_DIR}")
        return
    
    print(f"\nğŸ“„ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(html_files)} Ù…Ù‚Ø§Ù„")
    print("\nğŸ”„ Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„...\n")
    
    converted_articles = []
    failed_articles = []
    
    for i, filename in enumerate(html_files, 1):
        try:
            old_path = os.path.join(OLD_ARTICLES_DIR, filename)
            article_data = convert_article(old_path, TEMPLATE_PATH)
            converted_articles.append(article_data)
            
            print(f"   âœ… [{i}/{len(html_files)}] {filename}")
            
        except Exception as e:
            failed_articles.append((filename, str(e)))
            print(f"   âŒ [{i}/{len(html_files)}] {filename} - Ø®Ø·Ø£: {str(e)}")
    
    print("\n" + "=" * 60)
    print(f"âœ… ØªÙ… ØªØ­ÙˆÙŠÙ„ {len(converted_articles)} Ù…Ù‚Ø§Ù„ Ø¨Ù†Ø¬Ø§Ø­")
    
    if failed_articles:
        print(f"âŒ ÙØ´Ù„ ØªØ­ÙˆÙŠÙ„ {len(failed_articles)} Ù…Ù‚Ø§Ù„:")
        for filename, error in failed_articles:
            print(f"   - {filename}: {error}")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù data.js Ø¬Ø¯ÙŠØ¯
    if converted_articles:
        print("\nğŸ“ Ø¬Ø§Ø±ÙŠ ØªØ­Ø¯ÙŠØ« data.js...")
        update_data_js(converted_articles)
        print("âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« data.js")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØªØ­ÙˆÙŠÙ„!")
    print(f"\nğŸ“‚ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ: {NEW_ARTICLES_DIR}/")
    print("ğŸ“ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø¯Ø«Ø© ÙÙŠ: assets/js/data.js")
    print("\nğŸ’¡ Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ©: Ø´ØºÙ‘Ù„ generate_sitemap.py Ù„ØªØ­Ø¯ÙŠØ« Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…ÙˆÙ‚Ø¹")

def update_data_js(articles_data):
    """ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù data.js"""
    
    # Ø¨Ù†Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
    articles_js = "const articles = [\n"
    
    for article in sorted(articles_data, key=lambda x: x['date'], reverse=True):
        articles_js += f'''
  {{
    slug: "{article['slug']}",
    title: "{article['title'].replace('"', '\\"')}",
    date: "{article['date']}",
    cat: "{article['category']}",
    desc: "{article['description'].replace('"', '\\"')[:200]}",
    img: "{article['image']}",
    url: "articles/{article['slug']}.html"
  }},'''
    
    articles_js += "\n];"
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
    os.makedirs(os.path.dirname(DATA_JS_PATH), exist_ok=True)
    with open(DATA_JS_PATH, 'w', encoding='utf-8') as f:
        f.write(articles_js)

if __name__ == "__main__":
    # ØªØ«Ø¨ÙŠØª BeautifulSoup Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
    try:
        from bs4 import BeautifulSoup
    except ImportError:
        print("ğŸ“¦ Ø¬Ø§Ø±ÙŠ ØªØ«Ø¨ÙŠØª BeautifulSoup...")
        import subprocess
        subprocess.check_call(['pip', 'install', 'beautifulsoup4', '--break-system-packages'])
        from bs4 import BeautifulSoup
        print("âœ… ØªÙ… Ø§Ù„ØªØ«Ø¨ÙŠØª Ø¨Ù†Ø¬Ø§Ø­\n")
    
    convert_all_articles()
