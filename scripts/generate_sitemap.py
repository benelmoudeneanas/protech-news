#!/usr/bin/env python3
"""
Generate sitemap.xml automatically
ÙŠÙˆÙ„Ø¯ sitemap Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
"""

import os
import re
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom

# Configuration
BASE_URL = "https://protechdaily.online"
ARTICLES_DIR = "articles"
PAGES_DIR = "pages"
OUTPUT_FILE = "sitemap.xml"

def get_article_files():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª"""
    if not os.path.exists(ARTICLES_DIR):
        return []
    return [f for f in os.listdir(ARTICLES_DIR) if f.endswith('.html')]

def get_page_files():
    """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø§Ù„ØµÙØ­Ø§Øª"""
    files = []
    if os.path.exists(PAGES_DIR):
        files = [f for f in os.listdir(PAGES_DIR) if f.endswith('.html')]
    return files

def extract_date_from_file(filepath):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØªØ§Ø±ÙŠØ® Ù…Ù† Ø§Ù„Ù…Ù„Ù"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ§Ø±ÙŠØ® ÙÙŠ meta tags
            date_match = re.search(r'<time datetime="(\d{4}-\d{2}-\d{2})"', content)
            if date_match:
                return date_match.group(1)
            
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ datePublished
            date_match = re.search(r'"datePublished":\s*"(\d{4}-\d{2}-\d{2})"', content)
            if date_match:
                return date_match.group(1)
    except:
        pass
    
    # Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ¹Ø¯ÙŠÙ„
    try:
        mtime = os.path.getmtime(filepath)
        return datetime.fromtimestamp(mtime).strftime('%Y-%m-%d')
    except:
        return datetime.now().strftime('%Y-%m-%d')

def generate_sitemap():
    """ØªÙˆÙ„ÙŠØ¯ sitemap.xml"""
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¹Ù†ØµØ± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    urlset = Element('urlset')
    urlset.set('xmlns', 'http://www.sitemaps.org/schemas/sitemap/0.9')
    urlset.set('xmlns:news', 'http://www.google.com/schemas/sitemap-news/0.9')
    urlset.set('xmlns:xhtml', 'http://www.w3.org/1999/xhtml')
    urlset.set('xmlns:mobile', 'http://www.google.com/schemas/sitemap-mobile/1.0')
    urlset.set('xmlns:image', 'http://www.google.com/schemas/sitemap-image/1.1')
    
    # Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    url = SubElement(urlset, 'url')
    SubElement(url, 'loc').text = f"{BASE_URL}/"
    SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
    SubElement(url, 'changefreq').text = 'daily'
    SubElement(url, 'priority').text = '1.0'
    
    # Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
    main_pages = [
        ('about.html', 'monthly', '0.5'),
        ('contact.html', 'monthly', '0.5')
    ]
    
    for page, changefreq, priority in main_pages:
        if os.path.exists(page):
            url = SubElement(urlset, 'url')
            SubElement(url, 'loc').text = f"{BASE_URL}/{page}"
            SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
            SubElement(url, 'changefreq').text = changefreq
            SubElement(url, 'priority').text = priority
    
    # ØµÙØ­Ø§Øª Ø§Ù„Ø£Ù‚Ø³Ø§Ù…
    page_files = get_page_files()
    for page_file in page_files:
        url = SubElement(urlset, 'url')
        SubElement(url, 'loc').text = f"{BASE_URL}/pages/{page_file}"
        SubElement(url, 'lastmod').text = datetime.now().strftime('%Y-%m-%d')
        SubElement(url, 'changefreq').text = 'weekly'
        SubElement(url, 'priority').text = '0.8'
    
    # Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª (Ù…Ø±ØªØ¨Ø© Ù…Ù† Ø§Ù„Ø£Ø­Ø¯Ø« Ù„Ù„Ø£Ù‚Ø¯Ù…)
    article_files = get_article_files()
    
    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
    articles_with_dates = []
    for article_file in article_files:
        filepath = os.path.join(ARTICLES_DIR, article_file)
        article_date = extract_date_from_file(filepath)
        articles_with_dates.append((article_file, article_date))
    
    # ØªØ±ØªÙŠØ¨ Ø¹ÙƒØ³ÙŠ (Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹)
    articles_with_dates.sort(key=lambda x: x[1], reverse=True)
    
    # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ù‚Ø§Ù„Ø§Øª
    for article_file, article_date in articles_with_dates:
        url = SubElement(urlset, 'url')
        SubElement(url, 'loc').text = f"{BASE_URL}/articles/{article_file}"
        SubElement(url, 'lastmod').text = article_date
        SubElement(url, 'changefreq').text = 'weekly'
        SubElement(url, 'priority').text = '0.9'
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ XML Ù…Ù†Ø³Ù‚
    xml_str = minidom.parseString(tostring(urlset)).toprettyxml(indent='    ')
    
    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø³Ø·Ø± Ø§Ù„ÙØ§Ø±ØºØ©
    xml_str = '\n'.join([line for line in xml_str.split('\n') if line.strip()])
    
    # Ø§Ù„ÙƒØªØ§Ø¨Ø© ÙÙŠ Ø§Ù„Ù…Ù„Ù
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        f.write(xml_str)
    
    print(f"âœ… ØªÙ… ØªÙˆÙ„ÙŠØ¯ Sitemap: {OUTPUT_FILE}")
    print(f"   - 1 ØµÙØ­Ø© Ø±Ø¦ÙŠØ³ÙŠØ©")
    print(f"   - {len(main_pages)} ØµÙØ­Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©")
    print(f"   - {len(page_files)} ØµÙØ­Ø§Øª Ø£Ù‚Ø³Ø§Ù…")
    print(f"   - {len(article_files)} Ù…Ù‚Ø§Ù„")
    print(f"   - Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹: {1 + len(main_pages) + len(page_files) + len(article_files)} URL")
    
    return OUTPUT_FILE

if __name__ == "__main__":
    print("ğŸ—ºï¸  ProTech Sitemap Generator")
    print("=" * 60)
    generate_sitemap()
    print("\nâœ¨ ØªÙ…! Ø¬Ø§Ù‡Ø² Ù„Ù„Ø¥Ø±Ø³Ø§Ù„ Ø¥Ù„Ù‰ Google Search Console")
