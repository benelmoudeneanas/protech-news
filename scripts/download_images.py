#!/usr/bin/env python3
"""
Image Downloader for ProTech News
ÙŠÙ‚Ø±Ø£ Ø§Ù„ØµÙˆØ± Ù…Ù† data.jsØŒ ÙŠØ­Ù…Ù„Ù‡Ø§ØŒ ÙˆÙŠØ®Ø²Ù†Ù‡Ø§ Ù…Ø­Ù„ÙŠØ§Ù‹
"""

import os
import re
import hashlib
import requests
from urllib.parse import urlparse
from pathlib import Path

# Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
DATA_JS_PATH = "assets/js/data.js"
IMAGES_DIR = "assets/images/articles"
TEMP_DATA_JS = "assets/js/data.js.backup"

def ensure_directories():
    """Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª"""
    os.makedirs(IMAGES_DIR, exist_ok=True)

def parse_data_js():
    """Ù‚Ø±Ø§Ø¡Ø© ÙˆØªØ­Ù„ÙŠÙ„ data.js Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ØµÙˆØ±"""
    
    with open(DATA_JS_PATH, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¬Ù…ÙŠØ¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±
    # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†: img: "https://..." Ø£Ùˆ img: 'https://...'
    img_pattern = r'img\s*:\s*["\']([^"\']+)["\']'
    images = re.findall(img_pattern, content)
    
    return content, images

def get_image_extension(url):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù…ØªØ¯Ø§Ø¯ Ø§Ù„ØµÙˆØ±Ø© Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    parsed = urlparse(url)
    path = parsed.path
    
    # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·
    ext = os.path.splitext(path)[1]
    
    # Ø¥Ø°Ø§ Ù…Ø§ÙÙŠÙ‡Ø´ Ø§Ù…ØªØ¯Ø§Ø¯ØŒ Ø§Ø³ØªØ¹Ù…Ù„ .jpg ÙƒØ§ÙØªØ±Ø§Ø¶ÙŠ
    if not ext or ext not in ['.jpg', '.jpeg', '.png', '.webp', '.gif']:
        ext = '.jpg'
    
    return ext

def generate_local_filename(url):
    """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³Ù… Ù…Ù„Ù Ù…Ø­Ù„ÙŠ Ù…Ù† Ø§Ù„Ø±Ø§Ø¨Ø·"""
    # Ø§Ø³ØªØ¹Ù…Ø§Ù„ hash Ø¨Ø§Ø´ Ù†ØªØ¬Ù†Ø¨ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø£Ø³Ù…Ø§Ø¡
    url_hash = hashlib.md5(url.encode()).hexdigest()[:12]
    ext = get_image_extension(url)
    return f"article-{url_hash}{ext}"

def download_image(url, save_path):
    """ØªØ­Ù…ÙŠÙ„ ØµÙˆØ±Ø© Ù…Ù† Ø±Ø§Ø¨Ø·"""
    try:
        print(f"      â¬‡ï¸  ØªØ­Ù…ÙŠÙ„: {url[:60]}...")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=30, stream=True)
        response.raise_for_status()
        
        # Ø­ÙØ¸ Ø§Ù„ØµÙˆØ±Ø©
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        file_size = os.path.getsize(save_path) / 1024  # KB
        print(f"      âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„: {os.path.basename(save_path)} ({file_size:.1f} KB)")
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"      âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù…ÙŠÙ„: {str(e)[:50]}")
        return False
    except Exception as e:
        print(f"      âŒ Ø®Ø·Ø£: {str(e)[:50]}")
        return False

def process_images():
    """Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù…ÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØµÙˆØ±"""
    
    ensure_directories()
    
    # Ù‚Ø±Ø§Ø¡Ø© data.js
    content, images = parse_data_js()
    
    if not images:
        print("âš ï¸  Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØµÙˆØ± ÙÙŠ data.js")
        return
    
    print(f"\nğŸ“Š ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(images)} ØµÙˆØ±Ø©")
    print("=" * 70)
    
    # Ø®Ø±ÙŠØ·Ø© Ù„Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„ (Ø±Ø§Ø¨Ø· Ù‚Ø¯ÙŠÙ… -> Ø±Ø§Ø¨Ø· Ø¬Ø¯ÙŠØ¯)
    replacements = {}
    downloaded = 0
    skipped = 0
    failed = 0
    
    for i, img_url in enumerate(images, 1):
        print(f"\n[{i}/{len(images)}] Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±Ø©...")
        
        # ØªØ¬Ù†Ø¨ Ø§Ù„ØµÙˆØ± Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        if img_url.startswith('../') or img_url.startswith('assets/'):
            print(f"      â­ï¸  ØªØ®Ø·ÙŠ: ØµÙˆØ±Ø© Ù…Ø­Ù„ÙŠØ© Ø¨Ø§Ù„ÙØ¹Ù„")
            skipped += 1
            continue
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ù„ÙŠ
        local_filename = generate_local_filename(img_url)
        local_path = os.path.join(IMAGES_DIR, local_filename)
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„ØµÙˆØ±Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ø§Ù‹
        if os.path.exists(local_path):
            print(f"      âœ… Ù…ÙˆØ¬ÙˆØ¯ Ù…Ø³Ø¨Ù‚Ø§Ù‹: {local_filename}")
            skipped += 1
        else:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©
            if download_image(img_url, local_path):
                downloaded += 1
            else:
                failed += 1
                continue
        
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª
        # Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù…Ù† root: assets/images/articles/xxx.jpg
        new_url = f"assets/images/articles/{local_filename}"
        replacements[img_url] = new_url
    
    print("\n" + "=" * 70)
    print(f"ğŸ“¦ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù…ÙŠÙ„:")
    print(f"   âœ… ØªÙ… Ø§Ù„ØªØ­Ù…ÙŠÙ„: {downloaded}")
    print(f"   â­ï¸  Ù…ØªØ®Ø·Ø§Ø©: {skipped}")
    print(f"   âŒ ÙØ´Ù„Øª: {failed}")
    print(f"   ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(images)}")
    
    return content, replacements

def update_data_js(content, replacements):
    """ØªØ­Ø¯ÙŠØ« data.js Ø¨Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©"""
    
    if not replacements:
        print("\nâš ï¸  Ù„Ø§ ØªÙˆØ¬Ø¯ Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª Ù„Ù„ØªØ·Ø¨ÙŠÙ‚")
        return
    
    print("\n" + "=" * 70)
    print("ğŸ”„ ØªØ­Ø¯ÙŠØ« data.js...")
    
    # Ø­ÙØ¸ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
    backup_path = DATA_JS_PATH + ".backup"
    with open(DATA_JS_PATH, 'r', encoding='utf-8') as f:
        with open(backup_path, 'w', encoding='utf-8') as bf:
            bf.write(f.read())
    
    print(f"   ğŸ’¾ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_path}")
    
    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
    updated_content = content
    replaced_count = 0
    
    for old_url, new_url in replacements.items():
        if old_url in updated_content:
            updated_content = updated_content.replace(old_url, new_url)
            replaced_count += 1
            print(f"   âœ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„: {os.path.basename(new_url)}")
    
    # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø¯Ø«
    with open(DATA_JS_PATH, 'w', encoding='utf-8') as f:
        f.write(updated_content)
    
    print(f"\n   âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {replaced_count} Ø±Ø§Ø¨Ø· ÙÙŠ data.js")

def update_existing_html_files():
    """ØªØ­Ø¯ÙŠØ« Ù…Ù„ÙØ§Øª HTML Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©"""
    
    articles_dir = "articles"
    
    if not os.path.exists(articles_dir):
        return
    
    print("\n" + "=" * 70)
    print("ğŸ”„ ØªØ­Ø¯ÙŠØ« Ù…Ù„ÙØ§Øª HTML Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©...")
    
    html_files = [f for f in os.listdir(articles_dir) if f.endswith('.html')]
    
    if not html_files:
        print("   âš ï¸  Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª HTML")
        return
    
    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø§Ø³ØªØ¨Ø¯Ø§Ù„Ø§Øª Ù…Ù† data.js
    with open(DATA_JS_PATH, 'r', encoding='utf-8') as f:
        data_content = f.read()
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    local_images = re.findall(r'img\s*:\s*["\']assets/images/articles/([^"\']+)["\']', data_content)
    
    updated_count = 0
    
    for html_file in html_files:
        file_path = os.path.join(articles_dir, html_file)
        
        with open(file_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ± Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© Ø¨Ø§Ù„Ù…Ø­Ù„ÙŠØ©
        original_content = html_content
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† src="https://..." Ø£Ùˆ src='https://...'
        external_images = re.findall(r'src=["\']https://[^"\']+["\']', html_content)
        
        for ext_img in external_images:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ù„ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            for local_img in local_images:
                # Ø¥Ø°Ø§ ÙˆØ¬Ø¯Ù†Ø§ Ù…Ø·Ø§Ø¨Ù‚Ø©ØŒ Ù†Ø³ØªØ¨Ø¯Ù„
                local_path = f'../assets/images/articles/{local_img}'
                html_content = re.sub(
                    r'src=["\']https://[^"\']+' + re.escape(local_img[-20:]) + r'[^"\']*["\']',
                    f'src="{local_path}"',
                    html_content
                )
        
        # Ø­ÙØ¸ Ø¥Ø°Ø§ ØªØºÙŠØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if html_content != original_content:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
            updated_count += 1
            print(f"   âœ… Ù…Ø­Ø¯Ø«: {html_file}")
    
    print(f"\n   âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {updated_count} Ù…Ù„Ù HTML")

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    
    print("=" * 70)
    print("ğŸ–¼ï¸  ProTech Image Downloader & Localizer")
    print("=" * 70)
    
    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ data.js
    if not os.path.exists(DATA_JS_PATH):
        print(f"âŒ {DATA_JS_PATH} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    try:
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØ±
        content, replacements = process_images()
        
        # ØªØ­Ø¯ÙŠØ« data.js
        if replacements:
            update_data_js(content, replacements)
            
            # ØªØ­Ø¯ÙŠØ« Ù…Ù„ÙØ§Øª HTML Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            # update_existing_html_files()
        
        print("\n" + "=" * 70)
        print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ø¨Ù†Ø¬Ø§Ø­!")
        print("=" * 70)
        print("\nğŸ’¡ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
        print("   1. ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…Ø¬Ù„Ø¯: assets/images/articles/")
        print("   2. Ø±Ø§Ø¬Ø¹ data.js (Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: data.js.backup)")
        print("   3. Ø´ØºÙ„ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ù…ÙˆÙ„Ø¯: python3 scripts/auto_generate_from_data.py")
        print("   4. Commit & Push Ù„Ù„ØªØºÙŠÙŠØ±Ø§Øª")
        
    except Exception as e:
        print(f"\nâŒ Ø®Ø·Ø£: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
